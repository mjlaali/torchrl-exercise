{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc3f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import EnvBase, ParallelEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data import DiscreteTensorSpec, BinaryDiscreteTensorSpec, DiscreteTensorSpec, CompositeSpec, BoundedTensorSpec\n",
    "from torchrl.modules import MultiAgentMLP\n",
    "from tensordict import TensorDict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchrl.envs import set_exploration_type, ExplorationType\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9352fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tic Tac Toe \n",
    "# Environment asks each agent in each turn for a move (even the agent that is not its turn).\n",
    "# Environment accepts the move from active player and ignore the turn. \n",
    "# Regardless of which player is active, environment provided a reward for both agents\n",
    "\n",
    "DEFAULT_DEVICE = 'cpu'\n",
    "batch_size = [1]\n",
    "\n",
    "class TicTacToe(EnvBase):    \n",
    "    def __init__(self, seed=None, device=DEFAULT_DEVICE, *argv, **kwargs):\n",
    "        super().__init__(*argv, device=device, **kwargs)\n",
    "        self._make_spec()\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        \n",
    "    def _step(self, state_action: TensorDict):\n",
    "        batch_size = state_action.shape\n",
    "        # b x 1\n",
    "        turns = state_action[\"turn\"]\n",
    "        new_turns = torch.clone(turns)\n",
    "        \n",
    "        # b x 1\n",
    "        actions = state_action[self.action_key][:, 0] * (1 - turns) + state_action[self.action_key][:, 1] * turns\n",
    "\n",
    "        # b x 2 x 9\n",
    "        boards = torch.clone(state_action[\"board\"])\n",
    "        # b x 2\n",
    "        rewards = torch.zeros(self.batch_size + (2, 1))\n",
    "\n",
    "        # Check if the action points to a cell with zero value, otherwise it is an invalid move.\n",
    "        is_valids = torch.sum(F.one_hot(actions, 9) * boards, dim=-1) == 0\n",
    "        dones = torch.zeros(self.batch_size + (2, 1)).to(torch.bool)\n",
    "        \n",
    "        for idx in range(batch_size[0]):\n",
    "            turn = turns[idx].item()\n",
    "            action = actions[idx].item()\n",
    "            is_valid = is_valids[idx, turn].item()\n",
    "            if is_valid:\n",
    "                \n",
    "                boards[idx, turn, action] = 1\n",
    "                boards[idx, 1-turn, action] = 2\n",
    "\n",
    "                player_view = (boards[idx, turn] == 1).reshape(3, 3).to(torch.int)\n",
    "\n",
    "                row_win = torch.sum((torch.sum(player_view, dim=-1) == 3).long()) > 0\n",
    "                col_win = torch.sum((torch.sum(player_view, dim=-2) == 3).long()) > 0\n",
    "                main_diag_win = (torch.trace(player_view) == 3).long()\n",
    "                anti_diag_win = (torch.trace(torch.fliplr(player_view)) == 3).long()\n",
    "\n",
    "                won = torch.sum(\n",
    "                    (row_win + col_win + main_diag_win + anti_diag_win) > 0\n",
    "                ).to(torch.float)\n",
    "\n",
    "                rewards[idx, turn, 0] = won * 1\n",
    "                rewards[idx, 1-turn, 0] = 0\n",
    "                new_turns[idx] = 1 - turn\n",
    "            else:\n",
    "                rewards[idx, turn, 0] = 0\n",
    "\n",
    "            dones[idx, :, 0] = (torch.sum(boards[idx, :, :] != 0, dim=-1) == 9)\n",
    "        \n",
    "        next_state = TensorDict({\n",
    "                \"board\": boards,                        \n",
    "                \"reward\": rewards,\n",
    "                \"turn\": new_turns,\n",
    "                \"done\": dones,\n",
    "            },\n",
    "            state_action.shape,\n",
    "        )\n",
    "        return next_state\n",
    "\n",
    "    def _reset(self, tensordict: Optional[TensorDict]):   \n",
    "        batch_size = self.batch_size\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"board\": torch.zeros(batch_size + (2, 9)).long(),\n",
    "                \"turn\": torch.zeros(batch_size).long(), \n",
    "                \"done\": torch.zeros(batch_size + (2, 1)).bool(),\n",
    "            },\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    def _set_seed(self, seed: Optional[int]):\n",
    "        rng = torch.manual_seed(seed)\n",
    "        self.rng = rng\n",
    "\n",
    "    def _make_spec(self):\n",
    "        batch_size = self.batch_size\n",
    "        self.observation_spec = CompositeSpec(\n",
    "            {\n",
    "                \"board\": BoundedTensorSpec(\n",
    "                    minimum=0,\n",
    "                    maximum=2,\n",
    "                    shape=batch_size + (2, 9),\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "                \"turn\": DiscreteTensorSpec(n=2, shape=batch_size)\n",
    "            },\n",
    "            shape=batch_size\n",
    "        )\n",
    "        \n",
    "        self.state_spec = self.observation_spec.clone()\n",
    "        self.action_spec = DiscreteTensorSpec(n=9, shape=batch_size + (2, ))\n",
    "        self.reward_spec = BoundedTensorSpec(\n",
    "            minimum=-10, \n",
    "            maximum=1,\n",
    "            dtype=torch.float,\n",
    "            shape=batch_size + (2, 1)\n",
    "        )\n",
    "        self.done_spec = DiscreteTensorSpec(n=2, shape=batch_size + (2, 1), dtype=torch.bool)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26bf6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(batch_size=batch_size)\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5638f2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([1, 3, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        board: Tensor(shape=torch.Size([1, 3, 2, 9]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 3, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                board: Tensor(shape=torch.Size([1, 3, 2, 9]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                done: Tensor(shape=torch.Size([1, 3, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([1, 3, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                turn: Tensor(shape=torch.Size([1, 3]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([1, 3]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        turn: Tensor(shape=torch.Size([1, 3]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "    batch_size=torch.Size([1, 3]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rollout(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7478ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = env.step(TensorDict(\n",
    "    {\n",
    "        \"board\": torch.Tensor([[\n",
    "            [1, 1, 2, \n",
    "             2, 2, 1, \n",
    "             0, 2, 1],\n",
    "            [2, 2, 1, \n",
    "             1, 1, 2, \n",
    "             0, 1, 2]\n",
    "        ]]).long(),\n",
    "        \"turn\": torch.ones(1).long(), \n",
    "        \"action\": torch.Tensor([[6, 6]]).long(),\n",
    "    },\n",
    "    batch_size=[1]\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77b1814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ True]\n",
      "  [ True]]]\n",
      "[[[0.]\n",
      "  [1.]]]\n"
     ]
    }
   ],
   "source": [
    "print(out[\"next\", \"done\"].numpy())\n",
    "print(out[\"next\", \"reward\"].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d356953",
   "metadata": {},
   "source": [
    "# Create a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a3c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP, EGreedyWrapper\n",
    "from torchrl.data import OneHotDiscreteTensorSpec\n",
    "import torchrl.modules.tensordict_module as td_module\n",
    "from torchrl.envs import (\n",
    "    TransformedEnv, \n",
    "    Compose,\n",
    "    DoubleToFloat\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f453a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_act = 9\n",
    "n_state = 9\n",
    "n_inner = 20\n",
    "\n",
    "device = DEFAULT_DEVICE\n",
    "qvalue_net = nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"board\"].shape[-1], \n",
    "        n_agent_outputs=9,\n",
    "        n_agents=2,\n",
    "        centralised=False, \n",
    "        share_params=False,\n",
    "        device=device,\n",
    "        depth=4,\n",
    "        num_cells=32,\n",
    "        activation_class=torch.nn.LeakyReLU\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "780fb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenv = TransformedEnv(\n",
    "    env, \n",
    "    Compose(\n",
    "        DoubleToFloat(in_keys=[\"board\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "actor = td_module.QValueActor(\n",
    "    qvalue_net, \n",
    "    in_keys=[\"board\"], \n",
    "    action_space=env.action_spec,\n",
    ")\n",
    "stock_actor = EGreedyWrapper(\n",
    "    actor, \n",
    "    annealing_num_steps=1_000_000, \n",
    "    spec=env.action_spec, \n",
    "    eps_end=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e74b5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([1, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        board: Tensor(shape=torch.Size([1, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        turn: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "    batch_size=torch.Size([1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(tenv.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238ae456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1, 10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([1, 10, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        board: Tensor(shape=torch.Size([1, 10, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                board: Tensor(shape=torch.Size([1, 10, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                turn: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([1, 10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        turn: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "    batch_size=torch.Size([1, 10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "traj = tenv.rollout(10, policy=stock_actor)\n",
    "print(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66bb53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "0\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def visualize_traj(tenv, stock_actor, step_cnt=10, exploration_type=ExplorationType.MODE):\n",
    "    with torch.no_grad(), set_exploration_type(exploration_type):\n",
    "        traj = tenv.rollout(step_cnt, policy=stock_actor)\n",
    "    for idx in range(tenv.base_env.batch_size[0]):\n",
    "        for step in range(min(step_cnt, traj.shape[1])):\n",
    "            state = traj[idx, step]\n",
    "            turn = state[\"turn\"].item()\n",
    "            print(\"Next board:\")\n",
    "            print(state[\"next\", \"board\"][0].reshape(3, 3).cpu().numpy())\n",
    "            print(\"Trun:\")\n",
    "            print(turn)\n",
    "            print(\"Action:\")\n",
    "            print(state[\"action\"][turn].item())\n",
    "            print(\"Reward:\")\n",
    "            print(state[\"next\", \"reward\"].cpu().numpy())\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "visualize_traj(tenv, stock_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f3b76",
   "metadata": {},
   "source": [
    "# Build data set and train a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b24ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f8a8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = 1000000\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    tenv, \n",
    "    stock_actor, \n",
    "    frames_per_batch=20,\n",
    "    total_frames=total_frames,\n",
    "    reset_at_each_iter=True,\n",
    ")\n",
    "\n",
    "loss_fn = DQNLoss(\n",
    "    stock_actor, \n",
    "    action_space=tenv.action_spec,\n",
    "    delay_value=True,\n",
    ")\n",
    "\n",
    "updater = SoftUpdate(\n",
    "    loss_fn, eps=0.95\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(stock_actor.parameters(), lr=1e-4)\n",
    "\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyMemmapStorage(400), \n",
    "    batch_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91686b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759794cc7fd042818b41ac51a66ce9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check overfitting\n",
    "\n",
    "data = next(iter(collector))\n",
    "\n",
    "num_batches = 100\n",
    "utd = 256\n",
    "\n",
    "pbar = tqdm(total=num_batches)\n",
    "\n",
    "for _ in range(num_batches):\n",
    "    pbar.update(1)\n",
    "    losses = []    Â \n",
    "    for _ in range(utd):\n",
    "        loss_value = loss_fn(data)\n",
    "        loss_value[\"loss\"].backward()\n",
    "        losses.append(loss_value[\"loss\"].item())\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    pbar.set_description(f\"Avg loss = {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86c23cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "action:\n",
      "[3 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.08755484  0.01594245  0.15303257  0.16935739 -0.05269162  0.08200385\n",
      "   0.14890304  0.14318234 -0.04425136]\n",
      " [ 0.18890584  0.00999869  0.10499017  0.0314981   0.20820919  0.13446867\n",
      "   0.1673812   0.15329851  0.05362492]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[6 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.11025329 -0.04066113  0.16444457  0.14317235 -0.09383798  0.10736143\n",
      "   0.21789706  0.11730286  0.13621067]\n",
      " [ 0.16346502  0.03148388  0.16645524  0.08320613  0.17942204  0.13877127\n",
      "   0.15771054  0.1673688   0.07990478]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[6 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.11025329 -0.04066113  0.16444457  0.14317235 -0.09383798  0.10736143\n",
      "   0.21789706  0.11730286  0.13621067]\n",
      " [ 0.16346502  0.03148388  0.16645524  0.08320613  0.17942204  0.13877127\n",
      "   0.15771054  0.1673688   0.07990478]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[2 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.03212624  0.1262088   0.15452021  0.1460977  -0.02150267  0.12156062\n",
      "   0.1380115   0.1364037   0.03564981]\n",
      " [ 0.15825257  0.02111822  0.16347739  0.07332551  0.17072353  0.13402827\n",
      "   0.1396201   0.16089101  0.07079282]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[2 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.03212624  0.1262088   0.15452021  0.1460977  -0.02150267  0.12156062\n",
      "   0.1380115   0.1364037   0.03564981]\n",
      " [ 0.15825257  0.02111822  0.16347739  0.07332551  0.17072353  0.13402827\n",
      "   0.1396201   0.16089101  0.07079282]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 0.]]\n",
      "action:\n",
      "[2 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.03547744  0.1382787   0.1552074   0.14245014 -0.02493095  0.13816616\n",
      "   0.14865705  0.12473205  0.09255304]\n",
      " [ 0.15978421  0.05634278  0.11935222  0.07232682  0.18541747  0.17473482\n",
      "   0.17075002  0.14947924  0.09057301]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 0.]]\n",
      "action:\n",
      "[2 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.03547744  0.1382787   0.1552074   0.14245014 -0.02493095  0.13816616\n",
      "   0.14865705  0.12473205  0.09255304]\n",
      " [ 0.15978421  0.05634278  0.11935222  0.07232682  0.18541747  0.17473482\n",
      "   0.17075002  0.14947924  0.09057301]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 0.]]\n",
      "action:\n",
      "[8 4]\n",
      "reward\n",
      "[[1.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.21864924 -0.29233885  0.05472889 -0.05639724 -0.28204846  0.25212845\n",
      "   0.34086657  0.00432927  1.1353292 ]\n",
      " [ 0.16419299  0.06988251  0.12976864  0.09773088  0.19091287  0.18461189\n",
      "   0.18730232  0.15789966  0.11747675]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[7 4]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[ 0.00273734  0.13477069  0.13559836  0.0964165  -0.02736558  0.13546433\n",
      "   0.13539012  0.1375263   0.13512838]\n",
      " [ 0.18330625  0.13536492  0.18578938  0.18748687  0.19574848  0.18728644\n",
      "   0.18732734  0.18728766  0.1872954 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [2. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[1 8]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[-0.3570741   1.131855    0.15302838  0.09136193  0.42318586  0.13351905\n",
      "  -0.43464258  0.31412375 -1.1983002 ]\n",
      " [ 0.1852359   0.14999224  0.18653032  0.18749097  0.18774942  0.17646183\n",
      "   0.18207388  0.18663046  0.18794605]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [2. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[1 8]\n",
      "reward\n",
      "[[1.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[-0.3570741   1.131855    0.15302838  0.09136193  0.42318586  0.13351905\n",
      "  -0.43464258  0.31412375 -1.1983002 ]\n",
      " [ 0.1852359   0.14999224  0.18653032  0.18749097  0.18774942  0.17646183\n",
      "   0.18207388  0.18663046  0.18794605]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 1. 0.]\n",
      " [2. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[1 0]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[-0.31327954  1.0459652   0.1317845   0.13159889  0.38941735  0.14194518\n",
      "  -0.39415836  0.29526603 -1.0495466 ]\n",
      " [ 0.20036265  0.18517262  0.16068624  0.17503044  0.19200031  0.1746067\n",
      "   0.1865103   0.15944317  0.1852984 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 1. 0.]\n",
      " [2. 0. 2.]\n",
      " [1. 1. 1.]]\n",
      "action:\n",
      "[1 0]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n",
      "[[-0.31327954  1.0459652   0.1317845   0.13159889  0.38941735  0.14194518\n",
      "  -0.39415836  0.29526603 -1.0495466 ]\n",
      " [ 0.20036265  0.18517262  0.16068624  0.17503044  0.19200031  0.1746067\n",
      "   0.1865103   0.15944317  0.1852984 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show(sample):\n",
    "    for idx in range(sample.shape[0]):\n",
    "        print(\"board:\")\n",
    "        print(sample[idx][\"board\"][0, :].reshape(3, 3).numpy())\n",
    "        print(\"action:\")\n",
    "        print(sample[idx][\"action\"].numpy())\n",
    "        print(\"reward\")\n",
    "        print(sample[idx][\"next\", \"reward\"].numpy())\n",
    "        print(\"action_value\")\n",
    "        print(sample[idx][\"action_value\"].detach().numpy())\n",
    "        print()\n",
    "        print(\"----\\n\\n\")\n",
    "        \n",
    "res = actor(data)\n",
    "show(res.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2782b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(data)[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "841cbaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9abadb989804452a0d472d2ed28472c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(total=total_frames)\n",
    "\n",
    "utd = 16\n",
    "\n",
    "for i, data in enumerate(collector):\n",
    "    pbar.update(data.numel())\n",
    "    rb.extend(data.squeeze().to_tensordict().cpu())\n",
    "    losses = []\n",
    "    for _ in range(utd):\n",
    "        s = rb.sample().to(device)\n",
    "        loss_value = loss_fn(s)\n",
    "        loss_value[\"loss\"].backward()\n",
    "        losses.append(loss_value[\"loss\"].item())\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    \n",
    "    stock_actor.step()\n",
    "    updater.step()\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        with torch.no_grad(), set_exploration_type(ExplorationType.MODE):\n",
    "            sim = tenv.rollout(10, stock_actor)\n",
    "            re = sim[\"next\", \"reward\"].to(torch.float32).sum(dim=1).cpu().squeeze().numpy()\n",
    "            pbar.set_description(f\"Average reward = {re[0].item():.2f}, Avg loss = {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8123a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "6\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [2. 0. 0.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_traj(tenv, stock_actor, step_cnt=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdd718c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = actor(tenv.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10445d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([1, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        board: Tensor(shape=torch.Size([1, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        turn: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "    batch_size=torch.Size([1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00e7c8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = next(iter(collector))\n",
    "\n",
    "rb.extend(sample_data.squeeze().to_tensordict().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaaccb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board:\n",
      "[[2. 1. 1.]\n",
      " [1. 2. 1.]\n",
      " [2. 0. 2.]]\n",
      "action:\n",
      "[0 2]\n",
      "reward\n",
      "[[0.]\n",
      " [0.]]\n",
      "action_value\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'key \"action_value\" not found in TensorDict with keys [\\'action\\', \\'board\\', \\'collector\\', \\'done\\', \\'index\\', \\'next\\', \\'turn\\']'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/github/tensordict/tensordict/tensordict.py:3480\u001b[0m, in \u001b[0;36mTensorDictBase.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   3478\u001b[0m     idx_unravel \u001b[38;5;241m=\u001b[39m _unravel_key_to_tuple(index)\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx_unravel:\n\u001b[0;32m-> 3480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_unravel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNO_DEFAULT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (istuple \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m index) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m istuple \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m):\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;66;03m# empty tuple returns self\u001b[39;00m\n\u001b[1;32m   3483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/github/tensordict/tensordict/tensordict.py:4396\u001b[0m, in \u001b[0;36mTensorDict._get_tuple\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   4395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default):\n\u001b[0;32m-> 4396\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m default:\n\u001b[1;32m   4398\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m first\n",
      "File \u001b[0;32m~/github/tensordict/tensordict/tensordict.py:4392\u001b[0m, in \u001b[0;36mTensorDict._get_str\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   4390\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensordict\u001b[38;5;241m.\u001b[39mget(first_key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   4391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/github/tensordict/tensordict/tensordict.py:1324\u001b[0m, in \u001b[0;36mTensorDictBase._default_get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# raise KeyError\u001b[39;00m\n\u001b[0;32m-> 1324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1325\u001b[0m         TensorDictBase\u001b[38;5;241m.\u001b[39mKEY_ERROR\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1326\u001b[0m             key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   1327\u001b[0m         )\n\u001b[1;32m   1328\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'key \"action_value\" not found in TensorDict with keys [\\'action\\', \\'board\\', \\'collector\\', \\'done\\', \\'index\\', \\'next\\', \\'turn\\']'"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    show(rb.sample().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(collector):\n",
    "    if i == 10:\n",
    "        break\n",
    "    show(data.squeeze())\n",
    "    print(f\"----- {i} -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(data.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5924de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), set_exploration_type(ExplorationType.MODE):\n",
    "#     print(stock_actor(data)[\"action\"].numpy())\n",
    "    print(qvalue_net(data[\"board\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
