{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fc3f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import EnvBase, ParallelEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data import DiscreteTensorSpec, BinaryDiscreteTensorSpec, DiscreteTensorSpec, CompositeSpec, BoundedTensorSpec\n",
    "from torchrl.modules import MultiAgentMLP\n",
    "from tensordict import TensorDict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchrl.envs import set_exploration_type, ExplorationType\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9352fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tic Tac Toe \n",
    "# Environment asks each agent in each turn for a move (even the agent that is not its turn).\n",
    "# Environment accepts the move from active player and ignore the turn. \n",
    "# Regardless of which player is active, environment provided a reward for both agents\n",
    "\n",
    "DEFAULT_DEVICE = 'cpu'\n",
    "batch_size = [1]\n",
    "\n",
    "class TicTacToe(EnvBase):    \n",
    "    def __init__(self, seed=None, device=DEFAULT_DEVICE, *argv, **kwargs):\n",
    "        super().__init__(*argv, device=device, **kwargs)\n",
    "        self._make_spec()\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        \n",
    "    def _step(self, state_action: TensorDict):\n",
    "        batch_size = state_action.shape\n",
    "        # b x 1\n",
    "        turns = state_action[\"turn\"]\n",
    "        new_turns = torch.clone(turns)\n",
    "        \n",
    "        # b x 1\n",
    "        actions = state_action[self.action_key][:, 0] * (1 - turns) + state_action[self.action_key][:, 1] * turns\n",
    "\n",
    "        # b x 2 x 9\n",
    "        boards = torch.clone(state_action[\"board\"])\n",
    "        # b x 2\n",
    "        rewards = torch.zeros(self.batch_size + (2, 1))\n",
    "\n",
    "        # Check if the action points to a cell with zero value, otherwise it is an invalid move.\n",
    "        is_valids = torch.sum(F.one_hot(actions, 9) * boards, dim=-1) == 0\n",
    "        dones = torch.zeros(self.batch_size + (2, 1)).to(torch.bool)\n",
    "        \n",
    "        for idx in range(batch_size[0]):\n",
    "            turn = turns[idx].item()\n",
    "            action = actions[idx].item()\n",
    "            is_valid = is_valids[idx, turn].item()\n",
    "            if is_valid:\n",
    "                \n",
    "                boards[idx, turn, action] = 1\n",
    "                boards[idx, 1-turn, action] = 2\n",
    "\n",
    "                player_view = (boards[idx, turn] == 1).reshape(3, 3).to(torch.int)\n",
    "\n",
    "                row_win = torch.sum((torch.sum(player_view, dim=-1) == 3).long()) > 0\n",
    "                col_win = torch.sum((torch.sum(player_view, dim=-2) == 3).long()) > 0\n",
    "                main_diag_win = (torch.trace(player_view) == 3).long()\n",
    "                anti_diag_win = (torch.trace(torch.fliplr(player_view)) == 3).long()\n",
    "\n",
    "                won = torch.sum(\n",
    "                    (row_win + col_win + main_diag_win + anti_diag_win) > 0\n",
    "                ).to(torch.float)\n",
    "\n",
    "                rewards[idx, turn, 0] = won * 1\n",
    "                rewards[idx, 1-turn, 0] = 0\n",
    "                new_turns[idx] = 1 - turn\n",
    "            else:\n",
    "                rewards[idx, turn, 0] = 0\n",
    "\n",
    "            dones[idx, :, 0] = (torch.sum(boards[idx, :, :] != 0, dim=-1) == 9)\n",
    "        \n",
    "        next_state = TensorDict({\n",
    "                \"board\": boards,                        \n",
    "                \"reward\": rewards,\n",
    "                \"turn\": new_turns,\n",
    "                \"done\": dones,\n",
    "            },\n",
    "            state_action.shape,\n",
    "        )\n",
    "        return next_state\n",
    "\n",
    "    def _reset(self, tensordict: Optional[TensorDict]):   \n",
    "        batch_size = self.batch_size\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"board\": torch.zeros(batch_size + (2, 9)).long(),\n",
    "                \"turn\": torch.zeros(batch_size).long(), \n",
    "                \"done\": torch.zeros(batch_size + (2, 1)).bool(),\n",
    "            },\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    def _set_seed(self, seed: Optional[int]):\n",
    "        rng = torch.manual_seed(seed)\n",
    "        self.rng = rng\n",
    "\n",
    "    def _make_spec(self):\n",
    "        batch_size = self.batch_size\n",
    "        self.observation_spec = CompositeSpec(\n",
    "            {\n",
    "                \"board\": BoundedTensorSpec(\n",
    "                    minimum=0,\n",
    "                    maximum=2,\n",
    "                    shape=batch_size + (2, 9),\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "                \"turn\": DiscreteTensorSpec(n=2, shape=batch_size)\n",
    "            },\n",
    "            shape=batch_size\n",
    "        )\n",
    "        \n",
    "        self.state_spec = self.observation_spec.clone()\n",
    "        self.action_spec = DiscreteTensorSpec(n=9, shape=batch_size + (2, ))\n",
    "        self.reward_spec = BoundedTensorSpec(\n",
    "            minimum=-10, \n",
    "            maximum=1,\n",
    "            dtype=torch.float,\n",
    "            shape=batch_size + (2, 1)\n",
    "        )\n",
    "        self.done_spec = DiscreteTensorSpec(n=2, shape=batch_size + (2, 1), dtype=torch.bool)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26bf6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe(batch_size=batch_size)\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5638f2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([1, 3, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        board: Tensor(shape=torch.Size([1, 3, 2, 9]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 3, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                board: Tensor(shape=torch.Size([1, 3, 2, 9]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                done: Tensor(shape=torch.Size([1, 3, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([1, 3, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                turn: Tensor(shape=torch.Size([1, 3]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([1, 3]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        turn: Tensor(shape=torch.Size([1, 3]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "    batch_size=torch.Size([1, 3]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rollout(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7478ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = env.step(TensorDict(\n",
    "    {\n",
    "        \"board\": torch.Tensor([[\n",
    "            [1, 1, 2, \n",
    "             2, 2, 1, \n",
    "             0, 2, 1],\n",
    "            [2, 2, 1, \n",
    "             1, 1, 2, \n",
    "             0, 1, 2]\n",
    "        ]]).long(),\n",
    "        \"turn\": torch.ones(1).long(), \n",
    "        \"action\": torch.Tensor([[6, 6]]).long(),\n",
    "    },\n",
    "    batch_size=[1]\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b77b1814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ True]\n",
      "  [ True]]]\n",
      "[[[0.]\n",
      "  [1.]]]\n"
     ]
    }
   ],
   "source": [
    "print(out[\"next\", \"done\"].numpy())\n",
    "print(out[\"next\", \"reward\"].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d356953",
   "metadata": {},
   "source": [
    "# Create a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12a3c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP, EGreedyWrapper\n",
    "from torchrl.data import OneHotDiscreteTensorSpec\n",
    "import torchrl.modules.tensordict_module as td_module\n",
    "from torchrl.envs import (\n",
    "    TransformedEnv, \n",
    "    Compose,\n",
    "    DoubleToFloat\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f453a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_act = 9\n",
    "n_state = 9\n",
    "n_inner = 20\n",
    "\n",
    "device = DEFAULT_DEVICE\n",
    "qvalue_net = nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"board\"].shape[-1], \n",
    "        n_agent_outputs=9,\n",
    "        n_agents=2,\n",
    "        centralised=False, \n",
    "        share_params=False,\n",
    "        device=device,\n",
    "        depth=4,\n",
    "        num_cells=32,\n",
    "        activation_class=torch.nn.LeakyReLU\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "780fb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenv = TransformedEnv(\n",
    "    env, \n",
    "    Compose(\n",
    "        DoubleToFloat(in_keys=[\"board\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "actor = td_module.QValueActor(\n",
    "    qvalue_net, \n",
    "    in_keys=[\"board\"], \n",
    "    action_space=env.action_spec,\n",
    ")\n",
    "stock_actor = EGreedyWrapper(\n",
    "    actor, \n",
    "    annealing_num_steps=1_000_000, \n",
    "    spec=env.action_spec, \n",
    "    eps_end=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e74b5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([1, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        action_value: Tensor(shape=torch.Size([1, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        board: Tensor(shape=torch.Size([1, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        chosen_action_value: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        turn: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "    batch_size=torch.Size([1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(tenv.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238ae456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1, 10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([1, 10, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        board: Tensor(shape=torch.Size([1, 10, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                board: Tensor(shape=torch.Size([1, 10, 2, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1, 10, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                turn: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([1, 10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        turn: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "    batch_size=torch.Size([1, 10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "traj = tenv.rollout(10, policy=stock_actor)\n",
    "print(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d66bb53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "8\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "4\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "6\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 2.]\n",
      " [0. 2. 0.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 2.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "5\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 2.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "0\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 1. 2.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "1\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 1. 2.]\n",
      " [2. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "3\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 1. 2.]\n",
      " [2. 2. 1.]\n",
      " [1. 1. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[ 0.5]\n",
      " [-0.5]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def visualize_traj(tenv, stock_actor, step_cnt=10, exploration_type=ExplorationType.MODE):\n",
    "    with torch.no_grad(), set_exploration_type(exploration_type):\n",
    "        traj = tenv.rollout(step_cnt, policy=stock_actor)\n",
    "    for idx in range(tenv.base_env.batch_size[0]):\n",
    "        for step in range(min(step_cnt, traj.shape[1])):\n",
    "            state = traj[idx, step]\n",
    "            turn = state[\"turn\"].item()\n",
    "            print(\"Next board:\")\n",
    "            print(state[\"next\", \"board\"][0].reshape(3, 3).cpu().numpy())\n",
    "            print(\"Trun:\")\n",
    "            print(turn)\n",
    "            print(\"Action:\")\n",
    "            print(state[\"action\"][turn].item())\n",
    "            print(\"Reward:\")\n",
    "            print(state[\"next\", \"reward\"].cpu().numpy())\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "visualize_traj(tenv, stock_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f3b76",
   "metadata": {},
   "source": [
    "# Build data set and train a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b24ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8a8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = 1000000\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    tenv, \n",
    "    stock_actor, \n",
    "    frames_per_batch=20,\n",
    "    total_frames=total_frames,\n",
    "    reset_at_each_iter=True,\n",
    ")\n",
    "\n",
    "loss_fn = DQNLoss(\n",
    "    stock_actor, \n",
    "    action_space=tenv.action_spec,\n",
    "    delay_value=True,\n",
    ")\n",
    "\n",
    "updater = SoftUpdate(\n",
    "    loss_fn, eps=0.95\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(stock_actor.parameters(), lr=1e-4)\n",
    "\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyMemmapStorage(400), \n",
    "    batch_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91686b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6fb7ec2d1b4a8385f0f1d045a6e04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check overfitting\n",
    "\n",
    "data = next(iter(collector))\n",
    "\n",
    "num_batches = 100\n",
    "utd = 256\n",
    "\n",
    "pbar = tqdm(total=num_batches)\n",
    "\n",
    "for _ in range(num_batches):\n",
    "    pbar.update(1)\n",
    "    losses = []    \n",
    "    for _ in range(utd):\n",
    "        loss_value = loss_fn(data)\n",
    "        loss_value[\"loss\"].backward()\n",
    "        losses.append(loss_value[\"loss\"].item())\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    pbar.set_description(f\"Avg loss = {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c23cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "action:\n",
      "[5 1]\n",
      "reward\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "action_value\n",
      "[[-0.17484301 -0.02507906  0.03223633 -0.20505914  0.08394265  0.21963191\n",
      "   0.0335404   0.13915312 -0.08770215]\n",
      " [ 0.13596968  0.18114944  0.14345627  0.03492893  0.15403381  0.06375592\n",
      "   0.1271474  -0.00805739 -0.03087601]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[5 1]\n",
      "reward\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.3368715  -0.05212075  0.09145431 -0.30340815  0.13105103  0.15367018\n",
      "   0.14463265 -0.03134204 -0.11350302]\n",
      " [ 0.16112146  0.19017364  0.14953838  0.07242975  0.16655558  0.15610191\n",
      "   0.15169232 -0.018997    0.07356079]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[2 1]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8553548  -0.0128549   0.14454132 -0.6102624   0.10912043 -0.2465922\n",
      "   0.03302849 -0.8553393  -0.445301  ]\n",
      " [ 0.16942525  0.19958091  0.14828162  0.05101341  0.16112162  0.1602923\n",
      "   0.14103222 -0.01092182  0.16111578]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[2 1]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8553548  -0.0128549   0.14454132 -0.6102624   0.10912043 -0.2465922\n",
      "   0.03302849 -0.8553393  -0.445301  ]\n",
      " [ 0.16942525  0.19958091  0.14828162  0.05101341  0.16112162  0.1602923\n",
      "   0.14103222 -0.01092182  0.16111578]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[2 1]\n",
      "reward\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "action_value\n",
      "[[-0.8553548  -0.0128549   0.14454132 -0.6102624   0.10912043 -0.2465922\n",
      "   0.03302849 -0.8553393  -0.445301  ]\n",
      " [ 0.16942525  0.19958091  0.14828162  0.05101341  0.16112162  0.1602923\n",
      "   0.14103222 -0.01092182  0.16111578]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "action:\n",
      "[4 0]\n",
      "reward\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8048565   0.03106974  0.13151412 -0.5889084   0.1372832  -0.11820854\n",
      "  -0.00725219 -0.75267124 -0.46398556]\n",
      " [ 0.18805587  0.1879095   0.13620858  0.02555529  0.16757652  0.13189775\n",
      "   0.13850702 -0.01745202  0.15709977]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 2.]]\n",
      "action:\n",
      "[2 1]\n",
      "reward\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "action_value\n",
      "[[-0.8261582   0.03256285  0.18342492 -0.61992407  0.14324588  0.05574574\n",
      "   0.14075291 -0.63014036 -0.49789816]\n",
      " [ 0.16021535  0.17350295  0.15669693  0.05644108  0.13655657  0.12944914\n",
      "   0.15012936 -0.02885179 -0.04885248]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[2 3]\n",
      "reward\n",
      "[[ 0.]\n",
      " [-1.]]\n",
      "action_value\n",
      "[[-0.8734368   0.04416732  0.16172515 -0.6199084   0.15913902  0.14074002\n",
      "   0.14326903 -0.6345342  -0.5209535 ]\n",
      " [ 0.16668998  0.15438445  0.1694447   0.17229943  0.10089375  0.09692819\n",
      "   0.15807582 -0.0527533  -0.84332067]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[2 3]\n",
      "reward\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8734368   0.04416732  0.16172515 -0.6199084   0.15913902  0.14074002\n",
      "   0.14326903 -0.6345342  -0.5209535 ]\n",
      " [ 0.16668998  0.15438445  0.1694447   0.17229943  0.10089375  0.09692819\n",
      "   0.15807582 -0.0527533  -0.84332067]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [2. 0. 0.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 0]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8802595   0.33939642 -0.14813861 -0.6032301   0.3977251   0.1419222\n",
      "  -0.85670805 -0.8567178  -0.856734  ]\n",
      " [ 0.17411685  0.1629443   0.17229387  0.17230096  0.10774677  0.11487041\n",
      "   0.17228617 -0.06829692 -0.70919704]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [2. 0. 0.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 0]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8802595   0.33939642 -0.14813861 -0.6032301   0.3977251   0.1419222\n",
      "  -0.85670805 -0.8567178  -0.856734  ]\n",
      " [ 0.17411685  0.1629443   0.17229387  0.17230096  0.10774677  0.11487041\n",
      "   0.17228617 -0.06829692 -0.70919704]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [2. 0. 0.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 0]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8802595   0.33939642 -0.14813861 -0.6032301   0.3977251   0.1419222\n",
      "  -0.85670805 -0.8567178  -0.856734  ]\n",
      " [ 0.17411685  0.1629443   0.17229387  0.17230096  0.10774677  0.11487041\n",
      "   0.17228617 -0.06829692 -0.70919704]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [2. 0. 0.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 0]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.8802595   0.33939642 -0.14813861 -0.6032301   0.3977251   0.1419222\n",
      "  -0.85670805 -0.8567178  -0.856734  ]\n",
      " [ 0.17411685  0.1629443   0.17229387  0.17230096  0.10774677  0.11487041\n",
      "   0.17228617 -0.06829692 -0.70919704]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [2. 0. 0.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 0]\n",
      "reward\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "action_value\n",
      "[[-0.8802595   0.33939642 -0.14813861 -0.6032301   0.3977251   0.1419222\n",
      "  -0.85670805 -0.8567178  -0.856734  ]\n",
      " [ 0.17411685  0.1629443   0.17229387  0.17230096  0.10774677  0.11487041\n",
      "   0.17228617 -0.06829692 -0.70919704]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 0. 1.]\n",
      " [2. 0. 1.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 6]\n",
      "reward\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.79228914  0.3402403  -0.09776704 -0.58456856  0.36657888  0.14028618\n",
      "  -0.8077588  -0.786807   -0.79801506]\n",
      " [ 0.17266333  0.18113923  0.17799929  0.17461815  0.10450172  0.15465741\n",
      "   0.18639103 -0.08060724 -0.50186753]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 2. 1.]\n",
      " [2. 0. 1.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 6]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.85973006  0.20144494  0.02772921 -0.859728    0.231812   -0.8597183\n",
      "  -1.1331832  -1.3355517  -0.8597272 ]\n",
      " [ 0.19187331  0.18113986  0.18113093  0.18114196  0.11114036  0.18111986\n",
      "   0.19823274 -0.09157069 -0.4946715 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 2. 1.]\n",
      " [2. 0. 1.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 6]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.85973006  0.20144494  0.02772921 -0.859728    0.231812   -0.8597183\n",
      "  -1.1331832  -1.3355517  -0.8597272 ]\n",
      " [ 0.19187331  0.18113986  0.18113093  0.18114196  0.11114036  0.18111986\n",
      "   0.19823274 -0.09157069 -0.4946715 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 2. 1.]\n",
      " [2. 0. 1.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 6]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.85973006  0.20144494  0.02772921 -0.859728    0.231812   -0.8597183\n",
      "  -1.1331832  -1.3355517  -0.8597272 ]\n",
      " [ 0.19187331  0.18113986  0.18113093  0.18114196  0.11114036  0.18111986\n",
      "   0.19823274 -0.09157069 -0.4946715 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 2. 1.]\n",
      " [2. 0. 1.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 6]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.85973006  0.20144494  0.02772921 -0.859728    0.231812   -0.8597183\n",
      "  -1.1331832  -1.3355517  -0.8597272 ]\n",
      " [ 0.19187331  0.18113986  0.18113093  0.18114196  0.11114036  0.18111986\n",
      "   0.19823274 -0.09157069 -0.4946715 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "board:\n",
      "[[2. 2. 1.]\n",
      " [2. 0. 1.]\n",
      " [1. 1. 2.]]\n",
      "action:\n",
      "[4 6]\n",
      "reward\n",
      "[[-1.]\n",
      " [ 0.]]\n",
      "action_value\n",
      "[[-0.85973006  0.20144494  0.02772921 -0.859728    0.231812   -0.8597183\n",
      "  -1.1331832  -1.3355517  -0.8597272 ]\n",
      " [ 0.19187331  0.18113986  0.18113093  0.18114196  0.11114036  0.18111986\n",
      "   0.19823274 -0.09157069 -0.4946715 ]]\n",
      "\n",
      "----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show(sample):\n",
    "    for idx in range(sample.shape[0]):\n",
    "        print(\"board:\")\n",
    "        print(sample[idx][\"board\"][0, :].reshape(3, 3).numpy())\n",
    "        print(\"action:\")\n",
    "        print(sample[idx][\"action\"].numpy())\n",
    "        print(\"reward\")\n",
    "        print(sample[idx][\"next\", \"reward\"].numpy())\n",
    "        print(\"action_value\")\n",
    "        print(sample[idx][\"action_value\"].detach().numpy())\n",
    "        print()\n",
    "        print(\"----\\n\\n\")\n",
    "        \n",
    "res = actor(data)\n",
    "show(res.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2782b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3855, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(data)[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "841cbaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7692e970e75488cb957b8f46b0b0b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(total=total_frames)\n",
    "\n",
    "utd = 16\n",
    "\n",
    "for i, data in enumerate(collector):\n",
    "    pbar.update(data.numel())\n",
    "    rb.extend(data.squeeze().to_tensordict().cpu())\n",
    "    losses = []\n",
    "    for _ in range(utd):\n",
    "        s = rb.sample().to(device)\n",
    "        loss_value = loss_fn(s)\n",
    "        loss_value[\"loss\"].backward()\n",
    "        losses.append(loss_value[\"loss\"].item())\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    \n",
    "    stock_actor.step()\n",
    "    updater.step()\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        with torch.no_grad(), set_exploration_type(ExplorationType.MODE):\n",
    "            sim = tenv.rollout(10, stock_actor)\n",
    "            re = sim[\"next\", \"reward\"].to(torch.float32).sum(dim=1).cpu().squeeze().numpy()\n",
    "            pbar.set_description(f\"Average reward = {re[0].item():.2f}, Avg loss = {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8123a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next board:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "8\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "4\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "6\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 2.]\n",
      " [0. 2. 0.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "2\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[0. 0. 2.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "5\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 0. 2.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "0\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 1. 2.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "1\n",
      "Reward:\n",
      "[[ 0.]\n",
      " [-0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 1. 2.]\n",
      " [2. 2. 1.]\n",
      " [1. 0. 1.]]\n",
      "Trun:\n",
      "1\n",
      "Action:\n",
      "3\n",
      "Reward:\n",
      "[[-0.]\n",
      " [ 0.]]\n",
      "\n",
      "\n",
      "\n",
      "Next board:\n",
      "[[2. 1. 2.]\n",
      " [2. 2. 1.]\n",
      " [1. 1. 1.]]\n",
      "Trun:\n",
      "0\n",
      "Action:\n",
      "7\n",
      "Reward:\n",
      "[[ 0.5]\n",
      " [-0.5]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_traj(tenv, stock_actor, step_cnt=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd718c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = actor(tenv.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10445d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(collector))\n",
    "\n",
    "rb.extend(sample_data.squeeze().to_tensordict().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaccb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    show(rb.sample().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(collector):\n",
    "    if i == 10:\n",
    "        break\n",
    "    show(data.squeeze())\n",
    "    print(f\"----- {i} -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(data.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5924de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), set_exploration_type(ExplorationType.MODE):\n",
    "#     print(stock_actor(data)[\"action\"].numpy())\n",
    "    print(qvalue_net(data[\"board\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
